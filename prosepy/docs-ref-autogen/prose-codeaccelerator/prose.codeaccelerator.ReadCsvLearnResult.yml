### YamlMime:PythonClass
uid: prose.codeaccelerator.ReadCsvLearnResult
name: ReadCsvLearnResult
fullName: prose.codeaccelerator.ReadCsvLearnResult
module: prose.codeaccelerator
inheritances:
- builtins.object
summary: 'The result from attempting to learn a program for delimited file reading.


  (The result of calling <xref:prose.codeaccelerator.ReadCsvBuilder.learn>.)

  Only intended to be called internally.'
constructor:
  syntax: ReadCsvLearnResult(result)
methods:
- uid: prose.codeaccelerator.ReadCsvLearnResult.code
  name: code
  summary: Return the code that was learned.
  signature: code()
  return:
    description: 'A callable that accepts a single argument (the file name to read).

      Its string representation is a Python code to read the input file.'
attributes:
- uid: prose.codeaccelerator.ReadCsvLearnResult.column_count
  name: column_count
  summary: Number of columns that the learned program will generate in its output.
- uid: prose.codeaccelerator.ReadCsvLearnResult.delimiter
  name: delimiter
  summary: Single delimiter that the learned program will use to split columns if
    one exists; None otherwise.
- uid: prose.codeaccelerator.ReadCsvLearnResult.encoding
  name: encoding
  summary: The detected encoding of the sample file that will be used for subsequent
    reads.
- uid: prose.codeaccelerator.ReadCsvLearnResult.filename
  name: filename
  summary: Path to the input file used to learn.
- uid: prose.codeaccelerator.ReadCsvLearnResult.names
  name: names
  summary: List of column names learned from the input file that will be used in the
    output table.
- uid: prose.codeaccelerator.ReadCsvLearnResult.preview_data
  name: preview_data
  summary: Return the data obtained when running the learned program on the input
    file.
  return:
    description: The first `5` rows of the data.
    types:
    - <xref:Union>[[DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html),
      <xref:List>[[Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)]]
- uid: prose.codeaccelerator.ReadCsvLearnResult.skip_blank_lines
  name: skip_blank_lines
  summary: If True, skip over blank lines rather than interpreting as NaN values.

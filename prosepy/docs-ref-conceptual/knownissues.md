---
title: Known issues with Code Accelerator - Python
ms.date: 09/24/2018
ms.topic: conceptual
ms.service: prose-codeaccelerator
author: simmdan
ms.author: dsimmons
description: These are known issues with the current release of PROSE Code Accelerator for Python.
---

# Known issues with Code Accelerator
The following are issues that the PROSE team is aware of with the 1.0.0 release of Code Accelerator.  We are
working to fix them in a future release.  If you encounter issues not on this list, please report them on the
[prose-codeaccelerator Github repository](https://github.com/Microsoft/prose-codeaccelerator/issues).

## ReadCsvBuilder
- Only UTF-8 encoded files are supported.
  
## ReadFwfBuilder
- Only UTF-8 encoded files are supported.

### PySpark:
- Calling `ReadFwfLearnResult.data()` for a file with `\r\n` or `\r` line separators will cause `NameError: name
  'StringType' is not defined` exception.  As a workaround, add the following code to your session:
  ```python
  import prose.codeaccelerator as cx
  from pyspark.sql.types import StringType

  cx._functions.StringType = StringType
  ```

  
## ReadJsonBuilder
- Only UTF-8 encoded files are supported.

### Pandas:
- If the JSON has a single top level array of values (e.g., `{"top": [1, 2]}`), pandas' `json_normalize` throws
  `TypeError: must be str, not int`. Github issue: https://github.com/pandas-dev/pandas/issues/21536. This has been
  fixed since pandas 0.23.2, but Azure Data Studio uses pandas 0.22.0

- If the JSON has a single object of object of array, for instance:

  ```json
  {
      "name": "alan smith",
      "info": {
          "phones": [{
              "area": 111,
              "number": 2222
          }, {
              "area": 333,
              "number": 4444
          }]
      }
  }
  ```
  pandas' `json_normalize` throws `TypeError: string indices must be integers`. Github issue:
  https://github.com/pandas-dev/pandas/issues/22706

### PySpark:
- If the JSON is an array of values (e.g., `[1, 2, 3]`), `spark.read.json` throws `AnalysisException: 'Since Spark 2.3, the
  queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record
  column`. Note that array of objects is not affected.

## DetectTypesBuilder
- A numeric column formatted like `##.#`, where `#` represents a digit, is detected as a time if all the values are
  between `0.0` and `24.0` with no fractional part exceeding `.59`. Essentially if the numeric values all look like
  valid time values in the `HH.M` (2 digit hour, 1 or 2 digit minute values) format, then their type may be incorrectly
  identified as `time`, rather than as numeric.

### Pandas:
- If the code generated by the datatype detection APIs ever throws a `ValueError` (this could occur due to bugs in the
  datatype detection APIs, or due to data values of a form different than those that were encountered during learning),
  then it could result in a stack overflow from unbounded recursion. This is due to the way the `DataFrame.transform`
  function works in pandas: `ValueError` is used internally to signal a failure and handle it. In the process, it ends
  up with an unbounded recursion.

- Pandas DataFrame objects with non string-valued column identifiers are not supported. Such a DataFrame object commonly
  arises when a `pandas.DataFrame` is created from a list of values. Suggested workaround: rename the columns of the
  DataFrame by calling `str()` on the non string-valued column identifiers.

- When the input is a pandas DataFrame, and the `target` property on the `DetectTypesBuilder` instance is set to
  `pandas` or `auto`, if a column containing only integer values contains one more more `NA` values, then although the
  generated code promises to return an `int`, the pandas DataFrame object returned will have the values in that column
  coerced/promoted to `numpy.float64`. This is a "feature" of pandas and is documented
  [here](https://pandas.pydata.org/pandas-docs/stable/gotchas.html#nan-integer-na-values-and-na-type-promotions).

### PySpark:
- A column that contains only times (rather than datetimes) will result in the date values being set to `2000-01-01` in
  PySpark mode. This is a workaround to the fact that PySpark dataframes do not support dates earlier than `1970-01-01`.
  Further a bug in Python 3.6 causes dates earlier than `1970-01-02` to be handled incorrectly. To ensure reliable
  operation across platforms, time values without dates are represented with dates of `2000-01-01` in PySpark mode. The
  default date value for other targets remains `1900-01-01`, which is the default for Python.

